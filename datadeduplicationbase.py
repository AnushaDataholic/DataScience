# -*- coding: utf-8 -*-
"""Untitled32.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a0n5smn-0x2KF7BH1yJ9hereH9ec1Ss7
"""

pip install networkx rapidfuzz python-Levenshtein

#deduplicate pipeline
#install libraries

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, precision_recall_fscore_support
from rapidfuzz import fuzz
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx
from itertools import combinations
from collections import defaultdict
import math
import re

#Helper functions
def normalize_text(s):
  if pd.isna(s): return ""
  s = str(s).lower()
  s = re.sub(r'[^a-z0-9]+','',s)
  s = re.sub(r'\s+','',s).strip()
  return s

def jaccard_tokens(a,b):
  sa = set(a.split())
  sb = set(b.split())
  if not sa and not sb: return 1.0
  if not sa or not sb: return 0.0
  return len(sa&sb)/len(sa|sb)

#creating sample dataset

def make_demo():
  data = [
      {"id": "1", "title": "Learning from Data", "authors": "Y. Lecun", "year": "2013", "price": "" },
        {"id": "2", "title": "Learning from Data", "authors": "Yann LeCun", "year": "2013", "price": "" },
        {"id": "3", "title": "Deep Learning", "authors": "I. Goodfellow", "year": "2016", "price": "" },
        {"id": "4", "title": "Learning from Data: a practical approach", "authors": "Y. Le Cun", "year": "2013", "price": "" }
  ]
  return pd.DataFrame(data)

df = make_demo()

df.head()

df['title_norm'] = df['title'].map(normalize_text)
df['authors_norm'] = df['authors'].map(normalize_text)

df.head()

# blocking simple token-block by first word of title
blocks = defaultdict(list)
for idx, row in df.iterrows():
  key = (row['title_norm'].split()[0] if row['title_norm'] else "")
  blocks[key].append(row['id'])

blocks

# Produce candidate pairs with in each block
pairs = []
for key,ids in blocks.items():
  for a, b in combinations(ids, 2):
    pairs.append((a, b))

pairs

#map id -> row for quick loop
rows = {r['id']: r for r in df.to_dict('records')}

rows

# ---- Feature generation for each pair ----
def pair_features(a_id, b_id):
    a = rows[a_id]; b = rows[b_id]
    t1 = a['title_norm']; t2 = b['title_norm']
    au1 = a['authors_norm']; au2 = b['authors_norm']
    # Token Jaccard
    f_jacc_title = jaccard_tokens(t1, t2)
    f_jacc_auth = jaccard_tokens(au1, au2)
    # Fuzzy ratios
    f_fuzz_ratio = fuzz.ratio(t1, t2) / 100.0
    f_partial = fuzz.partial_ratio(t1, t2) / 100.0
    # Numeric difference (year)
    try:
        yr1 = int(a.get('year') or 0)
        yr2 = int(b.get('year') or 0)
        f_year_diff = abs(yr1 - yr2)
    except:
        f_year_diff = 999
    # TF-IDF char n-gram (we can compute vectorized outside for scale; using simple char-based sim here)
    # As an approximation: cosine on simple token counts
    vec = TfidfVectorizer(analyzer='char', ngram_range=(2,4))
    # Fit on the two strings only (for demo). In practice fit on entire column once.
    X = vec.fit_transform([t1, t2])
    f_cosine = float(cosine_similarity(X[0], X[1])[0,0])
    return {
        'id_a': a_id, 'id_b': b_id,
        'jacc_title': f_jacc_title,
        'jacc_auth': f_jacc_auth,
        'fuzz_ratio': f_fuzz_ratio,
        'fuzz_partial': f_partial,
        'year_diff': f_year_diff,
        'cosine_char': f_cosine
    }

feature_rows = [pair_features(a,b) for a,b in pairs]
feat_df = pd.DataFrame(feature_rows)
print('Feature_DF')
print(feat_df)

# ---- Labels (for supervised datasets) ----
# If dataset provides pair labels, use them. For demo we create labels manually:
# Let's mark pairs (1,2) and (1,4) as matches in demo
labels = []
for r in feature_rows:
  a,b = r['id_a'],r['id_b']
  match = 1 if ( (a=='1' and b in ('2','4')) or (a=='2' and b=='4') ) else 0
  labels.append(match)
feat_df['label'] = labels

feat_df

#Train/Test
X = feat_df.drop(columns=['id_a','id_b','label'])
y = feat_df['label']
if len(y.unique()) > 1:
    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42, test_size=0.4)
    clf = RandomForestClassifier(n_estimators=200, random_state=42)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print(classification_report(y_test, y_pred))
else:
  print("Not enough label variety in demo. Use a real ER dataset with pair labels")

# In production: compute features for all blocked candidate pairs and use clf.predict_proba to score.
# For demo, we'll assume threshold 0.5 on clf if present.
if 'clf' in locals():
  probs = clf.predict_proba(X)[:,1]
  feat_df['score'] = probs
else:
  feat_df['score'] = 0.0
  TH = 0.5
  matches = feat_df[feat_df['score'] >= TH]

# Build graph and find connected components
G = nx.Graph()
G.add_nodes_from(df['id'].tolist())
for _,row in matches.iterrows():
  G.add_edge(row['id_a'],row['id_b'])
components = list(nx.connected_components(G))
print("Clusters(connected components):",components)

#Deduplication reduction metric
orig_count = df.shape[0]
unique_after = len(components)
reduction = 1 - (unique_after/orig_count)
print(f"Records original:{orig_count}, unique after dedupe:{unique_after}, reduction:{reduction:.2%}")

#save dedup mapping
cluster_map = {}
for i, comp in enumerate(components):
  for rid in comp:
    cluster_map[id] = f"cluster{i}"
df['cluster'] = df['id'].map(cluster_map)
print(df)

