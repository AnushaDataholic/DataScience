# -*- coding: utf-8 -*-
"""EDA Techniques.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ge3CPMovhlq4zDmy8dMvC_4AbnSl9zp9
"""

from google.colab import drive
drive.mount('/content/drive')

"""Handling imbalanced dataset

1. Up Sampling
2. Down Sampling
"""

import numpy as np
import pandas as pd
# Set the random seed for reproducibility
np.random.seed(123)
n_samples = 1000
class_0_ratio = 0.9
n_class_0 = int(n_samples * class_0_ratio)
n_class_1 = n_samples - n_class_0

n_class_0, n_class_1

class_0 = pd.DataFrame({
    'feature_1' : np.random.normal(loc = 0, scale = 1, size = n_class_0),
    'feature_2' : np.random.normal(loc = 0, scale = 1, size = n_class_0),
    'target' : [0] * n_class_0
})
class_1 = pd.DataFrame({
    'feature_1' : np.random.normal(loc = 2, scale = 1, size = n_class_1),
    'feature_2' : np.random.normal(loc = 2, scale = 1, size = n_class_1),
    'target' : [1] * n_class_1
})

df = pd.concat([class_0, class_1]).reset_index(drop = True)

df.tail()

df.head()

df['target'].value_counts()

# up Sampling
df_minority = df[df['target'] == 1]
df_majority = df[df['target'] == 0]

from sklearn.utils import resample
df_minority_upsampled = resample(df_minority, replace = True, n_samples = len(df_majority), random_state = 42)

df_minority_upsampled.shape

df_minority_upsampled.head()

df.head()

df.shape

df_minority_upsampled.shape

df_upsampled = pd.concat([df_majority, df_minority_upsampled])

df_upsampled.shape

"""Down Sampling"""

import pandas as pd
# create random seed for reproducibility
np.random.seed(123)
# create a dataframe with two classes
n_samples = 1000
class_0_ratio = 0.9
n_class_0 = int(n_samples * class_0_ratio)
n_class_1 = n_samples - n_class_0

class_0 = pd.DataFrame({
    'feature_1' : np.random.normal(loc = 0, scale = 1, size = n_class_0),
    'feature_2' : np.random.normal(loc = 0, scale = 1, size = n_class_0),
    'target' : [0] * n_class_0
})
class_1 = pd.DataFrame({
    'feature_1' : np.random.normal(loc = 2, scale = 1, size = n_class_1),
    'feature_2' : np.random.normal(loc = 2, scale = 1, size = n_class_1),
    'target' : [1] * n_class_1
})
df = pd.concat([class_0, class_1]).reset_index(drop = True)
print(df['target'].value_counts())

# down samling
df_minority = df[df['target'] == 1]
df_majority = df[df['target'] == 0]

from sklearn.utils import resample
df_majority_downsampled = resample(df_majority, replace= True, n_samples = len(df_minority), random_state = 42)

df_downsampled = pd.concat([df_majority_downsampled, df_minority])

df_downsampled.shape

"""Building Machine Learning Pipelines: Data Analysis Phase
In this and the upcoming videos we will focus on creating Machine Learning Pipelines considering all the life cycle of a Data Science Projects. This will be important for professionals who have not worked with huge dataset.

Project Name: House Prices: Advanced Regression Techniques
The main aim of this project is to predict the house price based on various features which we will discuss as we go ahead

Dataset to downloaded from the below link

https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data

All the Lifecycle In A Data Science Projects
Data Analysis
Feature Engineering
Feature Selection
Model Building
Model Deployment
"""

# Commented out IPython magic to ensure Python compatibility.
from pandas.core.indexes.base import InvalidIndexError
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

pd.pandas.set_option('display.max_columns', None)

dataset = pd.read_csv('/content/drive/MyDrive/DS_Datasets/house-prices-advanced-regression-techniques/train.csv')

print(dataset.shape)

dataset.head()

"""In Data Analysis we analyze the below stuff

1. Missing values
2. All numerical variables
3. Distribution of numerical variables
4. categorical variables
5. cardinality of categorical variables
6. outliers
7. relationship between independent and dependent feature
"""

# missing values

features_with_na = [features for features in dataset.columns if dataset[features].isnull().sum() > 1]
for feature in features_with_na:
  print(feature, np.round(dataset[feature].isnull().mean(),4), '% missing values')

"""Since there are many missing values, we need to find the relationship between missing values and salesprice"""

for feature in features_with_na:
    data = dataset.copy()

    # let's make a variable that indicates 1 if the observation was missing or zero otherwise
    data[feature] = np.where(data[feature].isnull(), 1, 0)

    # let's calculate the mean SalePrice where the information is missing or present
    data.groupby(feature)['SalePrice'].median().plot.bar()
    plt.title(feature)
    plt.show()

"""Here with the relation between missing values and the dependent variable is clearly available. so we need to replace these nan values with something meaningful which we will do in feature engineering section

From the above dataset some features like id is not requires
"""

print("Id of houses {}".format(len(dataset.Id)))

"""Numerical variables"""

# list of numerical variables
numerical_features = [feature for feature in dataset.columns if dataset[feature].dtypes != 'O']

print('Number of numerical variables: ', len(numerical_features))

# visualise the numerical variables
dataset[numerical_features].head()

"""Temporal Variables(Eg: Datetime Variables)
From the Dataset we have 4 year variables. We have extract information from the datetime variables like no of years or no of days. One example in this specific scenario can be difference in years between the year the house was built and the year the house was sold. We will be performing this analysis in the Feature Engineering which is the next video.
"""

# list all variables that contain year information
year_feature = [feature for feature in numerical_features if 'Yr' in feature or 'Year' in feature ]
year_feature

for feature in year_feature:
  print(feature, dataset[feature].unique())

# lets analyze temporal datetime variables
# we will check whether there is relation between year the house is sold and slaes price
dataset.groupby('YrSold')['SalePrice'].median().plot()
plt.xlabel('Year sold')
plt.ylabel('Median house price')
plt.title('House price vs year sold')

year_feature

# we will compare the difference between all years feature with sale price
for feature in year_feature:
  if feature != 'YrSold':
    data = dataset.copy()
    data[feature] = data['YrSold'] - data[feature]
    plt.scatter(data[feature], data['SalePrice'])
    plt.xlabel(feature)
    plt.ylabel('SalePrice')
    plt.show()

# Numerical variables are two types
# continuous variables and discrete variables

discrete_feature=[feature for feature in numerical_features if len(dataset[feature].unique())<25 and feature not in year_feature+['Id']]
print("Discrete Variables Count: {}".format(len(discrete_feature)))

discrete_feature

dataset[discrete_feature].head()

# let's find relation between them and sales price
for feature in discrete_feature:
  data = dataset.copy()
  data.groupby(feature)['SalePrice'].median().plot.bar()
  plt.xlabel(feature)
  plt.ylabel('SalePrice')
  plt.title(feature)
  plt.show()

"""Continuous variable"""

continuous_feature = [feature for feature in numerical_features if feature not in discrete_feature + year_feature + ['Id']]
print("Continuous feature Count {}".format(len(continuous_feature)))

# lets analyse the continuous feature by creating histograms to understand
for feature in continuous_feature:
  data = dataset.copy()
  data[feature].hist(bins = 25)
  plt.xlabel(feature)
  plt.ylabel("Count")
  plt.title(feature)
  plt.show()

# we will be using lograthmic transformation

for feature in continuous_feature:
    data=dataset.copy()
    if 0 in data[feature].unique():
        pass
    else:
        data[feature]=np.log(data[feature])
        data['SalePrice']=np.log(data['SalePrice'])
        plt.scatter(data[feature],data['SalePrice'])
        plt.xlabel(feature)
        plt.ylabel('SalesPrice')
        plt.title(feature)
        plt.show()

"""Outliers"""

for feature in continuous_feature:
  data = dataset.copy()
  if 0 in data[feature].unique():
    pass
  else:
    data[feature] = np.log(data[feature])
    data.boxplot(column = feature)
    plt.ylabel(feature)
    plt.title(feature)
    plt.show()

"""Categorical variables"""

categorical_features = [feature for feature in dataset.columns if data[feature].dtype == 'O']
categorical_features

dataset[categorical_features].head()

for feature in categorical_features:
  print('The feature is {} and number of categories are {}'.format(feature, len(dataset[feature].unique())))

"""find out the relationship between categorical features and salesprice"""

for feture in categorical_features:
  data = dataset.copy()
  data.groupby(feature)['SalePrice'].median().plot.bar()
  plt.xlabel(feature)
  plt.ylabel('SalePrice')
  plt.title(feature)
  plt.show()

"""Advanced housing prices - feature engineering

the main aim to predict the house price based on various features we will discuss as we go a head

we will be performing feature engineering including below steps

1. misssing values
2. temporal variables
3. categorical variables : remove labels
4. standardise the values and labels to same range
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
pd.pandas.set_option('display.max_columns', None)

# always remember there is a chance of data leakage so we need to split the data first and then apply feature engineering
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(dataset,dataset['SalePrice'], test_size = 0.1, random_state = 0)

X_train.shape, X_test.shape

"""Missing values

let us capture all the nan values
let's handle categorical features which are missing first
"""

features_nan = [feature for feature in dataset.columns if dataset[feature] .isnull().sum()>1 and dataset[feature].dtypes == 'O']

for feature in features_nan:
  print("{}: {}% missing values".format(feature, np.round(dataset[feature].isnull().mean(),4)))

# Replcae missing values with new label
def replace_cat_feature(dataset, features_nan):
  data = dataset.copy()
  data[features_nan] = data[features_nan].fillna('Missing')
  return data
dataset = replace_cat_feature(dataset, features_nan)
dataset[features_nan].isnull().sum()

dataset.head()

#Now lets check numerical features containing missing values
numerical_with_nan = [feature for feature in dataset.columns if dataset[feature].isnull().sum() > 1 and dataset[feature].dtypes != 'O']
# printing numerical nan variables
for feature in numerical_with_nan:
  print("{}: {}% missing value".format(feature, np.round(dataset[feature].isnull().mean()), 4))

## Replacing the numerical Missing Values

for feature in numerical_with_nan:
    ## We will replace by using median since there are outliers
    median_value=dataset[feature].median()

    ## create a new feature to capture nan values
    dataset[feature+'nan']=np.where(dataset[feature].isnull(),1,0)
    dataset[feature].fillna(median_value,inplace=True)

dataset[numerical_with_nan].isnull().sum()

dataset[numerical_with_nan]

dataset.isnull().sum()

dataset.head()

# Temporal variables [date time variables]

for feature in ['YearBuilt','YearRemodAdd','GarageYrBlt']:
  dataset[feature] = dataset['YrSold']- dataset[feature]

dataset.head()

dataset[['YearBuilt','YearRemodAdd','GarageYrBlt']].head()

"""Numerical variables

since the numerical variables are skewed we perform log normal distribution
"""

dataset.head()

import numpy as np
num_features=['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']
for feature in num_features:
  dataset[feature]= np.log(dataset[feature])

dataset.head()

"""Handling rare categorical feature

we will remove categorical variables that are present in less than 1% of the observations
"""

categorical_features = [feature for feature in dataset.columns if dataset[feature].dtype == 'O']

categorical_features

for feature in categorical_features:
  temp = dataset.groupby(feature)['SalePrice'].count()/len(dataset)
  temp_df = temp[temp > 0.01].index
  dataset[feature] = np.where(dataset[feature].isin(temp_df),dataset[feature],'Rare_var')

dataset.head(100)

for feature in categorical_features:
  labels_ordered = dataset.groupby([feature])['SalePrice'].mean().sort_values().index
  labels_ordered = {k:i for i,k in enumerate(labels_ordered, 0)}
  dataset[feature] = dataset[feature].map(labels_ordered)

dataset.head(10)

scaling_feature = [feature for feature in dataset.columns if feature not in ['Id','SalePrice']]

scaling_feature

dataset.head()

"""Feature Scaling"""

feature_scale = [feature for feature in dataset.columns if feature not in ['Id','SalePrice']]
from sklearn.preprocessing import  MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(dataset[feature_scale])

scaler.transform(dataset[feature_scale])

# transform the train test set, and add on the Id and saleprice variables
data = pd.concat([dataset[['Id','SalePrice']].reset_index(drop = True),
                  pd.DataFrame(scaler.transform(dataset[feature_scale]), columns = feature_scale)], axis = 1)

data.head()

dataset.head()

"""Feature Selection

Feature Selection Advanced House Price Prediction
The main aim of this project is to predict the house price based on various features which we will discuss as we go ahead

Dataset to downloaded from the below link
https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data
"""

# Commented out IPython magic to ensure Python compatibility.
from ast import increment_lineno
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

# for feature selection
from sklearn.linear_model import Lasso
from sklearn.feature_selection import  SelectFromModel

pd.pandas.set_option('display.max_columns', None)

data.head()

y_train  = data[['SalePrice']]

X_train = data.drop(['Id','SalePrice'], axis = 1)

### Apply Feature Selection
# first, I specify the Lasso Regression model, and I
# select a suitable alpha (equivalent of penalty).
# The bigger the alpha the less features that will be selected.

# Then I use the selectFromModel object from sklearn, which
# will select the features which coefficients are non-zero

feature_sel_model = SelectFromModel(Lasso(alpha=0.005, random_state=0)) # remember to set the seed, the random state in this function
feature_sel_model.fit(X_train, y_train)

feature_sel_model.get_support()

# lets print the number of total selected features
# this is how we can make list of the selected features
selected_feat = X_train.columns[(feature_sel_model.get_support())]

#lets print some stats
print('total features : {}'.format(X_train.shape[1]))
print('Selected features : {}'.format(len(selected_feat)))
print('features with coefficients shrank to zero: {}'.format(
    np.sum(feature_sel_model.estimator_.coef_ == 0)))

selected_feat

X_train =X_train[selected_feat]

X_train.head()

