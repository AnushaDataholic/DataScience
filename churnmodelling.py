# -*- coding: utf-8 -*-
"""ChurnModelling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1520Hd1B3d2HnnennFQhHzfx7OeyEt48X
"""

from google.colab import drive
drive.mount('/content/drive')

# standard imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

#SKlearn
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, MinMaxScaler,OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

#Models
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB

#Metrics and utilities
from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score,roc_auc_score, confusion_matrix, classification_report
import joblib
import warnings
warnings.filterwarnings('ignore')

#Display settings
sns.set_theme(style = 'whitegrid', palette = 'muted', font_scale= 1.05)
pd.set_option('display.max_columns',200)

#load dataset
df = pd.read_csv('/content/drive/MyDrive/DS_Datasets/customer_data.csv')
print("shape : " ,df.shape)
df.head()

"""Initial Inspection"""

#Info and summary
display(df.info())
display(df.describe(include = 'all').T)

#Missing values
print("\n missing values per column")
print(df.isnull().sum)

#Target distribution
print("\nChurn value counts:")
print(df['churn'].value_counts(normalize = False))
print("\nChurn proportion:")
print(df['churn'].value_counts(normalize = True))

"""Step 4 Exploratory data analysis

we'll look at distributions of numeric features, relationships with churn and categorical features
"""

num_cols = ['credit_score','age','tenure','balance','products_number','estimated_salary']
# Distribution plots

for col in num_cols:
  plt.figure(figsize=(8,3))
  sns.kdeplot(data = df, x = col, hue = 'churn', fill = True, common_norm =False,alpha = 0.5)
  plt.title(f'{col} Distribution by churn')
  plt.tight_layout()
  plt.show()

#pairplot
sns.pairplot(df.sample(frac = 0.2, random_state=42),
             vars = ['credit_score','age','balance','estimated_salary'],
             hue = 'churn', diag_kind = 'kde', plot_kws = {'alpha':0.5})
plt.suptitle('pairwise relationships among key features', y = 1.02)
plt.show()

sns.pairplot(df)

# violin plot for age
plt.figure(figsize = (8,4))
sns.violinplot(data = df, x = 'churn', y = 'age', inner = 'quart', palette = 'Set2')
plt.title('Age distribution by churn')
plt.show()

#Tenure distribution
plt.figure(figsize= (10,4))
sns.countplot(data = df, x = 'tenure', hue = 'churn', palette = 'coolwarm')
plt.title('Tenure ditribution by churn')
plt.show()

#Gender vs churn donut chart
gender_counts = df.groupby('gender')['churn'].value_counts(normalize = True).unstack().fillna(0)
for gender in gender_counts.index:
  plt.figure(figsize=(5,5))
  plt.pie(gender_counts.loc[gender], labels=['No Churn', 'Churn'],
            autopct='%1.1f%%', startangle=90, colors=['Red', 'Blue'])
  center = plt.Circle((0,0),0.70, fc = 'white')
  fig = plt.gcf()
  fig.gca().add_artist(center)
  plt.title(f'churn distribution for {gender}' )
  plt.tight_layout()
  plt.show()

#Categorical Features
cat_cols = ['country', 'gender','credit_card', 'active_member']
for c in cat_cols:
  plt.figure(figsize=(8,3))
  sns.countplot(data = df, x = c, hue = 'churn')
  plt.title(f'{c} vs churn')
  plt.xticks(rotation = 20)
  plt.tight_layout()
  plt.show()

# heatmap visualization
plt.figure(figsize=(10,6))
vars_to_plot = ['credit_score', 'age','tenure', 'balance', 'estimated_salary']
sns.heatmap(df[vars_to_plot].corr(), annot=True, fmt='.2f', cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Matrix (Numeric Features)')
plt.show()

# numeric data correlation heatmap
numeric_data = df[num_cols]
corr = numeric_data.corr()
corr.style.background_gradient(cmap = 'coolwarm')

# balance vs products scatter
plt.figure(figsize=(7,5))
sns.scatterplot(data = df, x = 'products_number', y = 'balance', hue= 'churn', alpha = 0.7)
plt.title('balance vs number of products by churn')
plt.show()

# aggregate churn rate per number of products
churn_rate = df.groupby('products_number')['churn'].mean().reset_index()
plt.figure(figsize=(8,5))
sns.barplot(data = churn_rate, x = 'products_number', y = 'churn', palette = 'viridis')
plt.xlabel('number of products')
plt.ylabel('churn rate')
plt.title('churn rate vs number of products')
plt.show()

"""Step 5 Feature engineering

create meaningful features per product, age group, salary bracket and handle missing values
"""

#feature engineering examples
df_fe = df.copy()

# balance per product
df_fe['balance_per_product'] = df_fe['balance']/(df_fe['products_number'].replace(0, np.nan))
df_fe['balance_per_product'].fillna(0, inplace = True)

#Salary balance ratio
df_fe['salary_balance_ratio'] = df_fe['estimated_salary']/(df_fe['balance'].replace(0, np.nan))
df_fe['salary_balance_ratio'].replace([np.inf,-np.inf],np.nan, inplace = True)
df_fe['salary_balance_ratio'].fillna(df_fe['salary_balance_ratio'].median(), inplace= True)

#Age group
bins = [0,25,35,45,55,65,100]
labels = ['<25','25-34','35-44','45-54','55-64','65+']
df_fe['age_group'] = pd.cut(df_fe['age'], bins = bins, labels = labels)

#Tenure bucket
df_fe['tenure_bucket'] = pd.cut(df_fe['tenure'], bins=[-1,0,2,5,10,100], labels=['0','1-2','3-5','6-10','10+'])

#flag high balance
df_fe['high_balance'] = (df_fe['balance'] > df_fe['balance'].quantile(0.75)).astype(int)

# Quick checks
df_fe[['balance_per_product','salary_balance_ratio','age','age_group','tenure','tenure_bucket','high_balance']].head()

"""Step 6 preprocessing - encoding andd scaling

we'll build a preprocessing pipeline that encodes categorical features and scales numerical ones
"""

# Define features and target
target = 'churn'
drop_cols = ['customer_id']
features = [c for c in df_fe.columns if c not in [target] + drop_cols]

numeric_features = ['credit_score','age','tenure','balance','products_number','estimated_salary',
                    'balance_per_product','salary_balance_ratio']
categorical_features = ['country','gender','credit_card','active_member','age_group','tenure_bucket','high_balance']

df_fe[categorical_features]= df_fe[categorical_features].astype('object')
numeric_transformer = Pipeline([
    ('imputer',SimpleImputer(strategy = 'median')),
    ('scaler', StandardScaler())
])
categorical_transformer  = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot',OneHotEncoder(handle_unknown='ignore',sparse_output= False))
])
preprocessor = ColumnTransformer([
    ('num', numeric_transformer,numeric_features),
    ('cat',categorical_transformer,categorical_features)
])

print('Numeric features:', numeric_features)
print('Categorical features:', categorical_features)

df_fe.head()

"""step 7 train test split"""

from re import X
X = df_fe[features]
y = df_fe[target]
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, stratify = y, random_state= 42)
print('X_train shape:', X_train.shape)
print('X_test shape:', X_test.shape)
print('Train churn proportion:', y_train.mean())
print('Test churn proportion:', y_test.mean())

"""Step8: train multiple models with a pipeline and compare using cross- validation"""

models = {
    'LogisticRegression': LogisticRegression(max_iter=500),
    'RandomForest': RandomForestClassifier(n_estimators= 200, random_state= 42),
    'GradientBoosting': GradientBoostingClassifier(n_estimators= 200, random_state= 42),
    'AdaBoost': AdaBoostClassifier(n_estimators= 200, random_state=42),
    'SVC':SVC(probability=True, random_state=42)
}

cv = StratifiedKFold(n_splits=5,shuffle = True, random_state= 42)
results = {}

for name, model in models.items():
  pipe = Pipeline(steps = [('preprocessor', preprocessor),
                           ('classifier', model)])
  scores = cross_val_score(pipe, X_train,y_train, cv = cv, scoring = 'roc_auc', n_jobs = -1)
  results[name] = scores
  print(f"{name} AUC: Mean = {scores.mean():.4f} Std = {scores.std():.4f}")

#Boxplot of CV AUC scores
plt.figure(figsize=(8,5))
sns.boxplot(data=[results[m] for m in list(results.keys())])
plt.xticks(ticks=range(len(results)), labels=list(results.keys()))
plt.ylabel('ROC AUC')
plt.title('Model comparision(cross-validated ROC AUC)')
plt.show()

"""step 9 Fit best model on full train set and evaluate test set

select the best model by men CV AUC above and standard metrics on the best set
"""

# choose best model automatically by mean AUC
best_name = max(results.keys(), key = lambda k: results[k].mean())
best_name, results[best_name].mean()

X_test.head()

best_model = models[best_name]
best_pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                                ('classifier', best_model)])
best_pipeline.fit(X_train, y_train)

#predictions
y_pred = best_pipeline.predict(X_test)
y_proba = best_pipeline.predict_proba(X_test)[:,1]

#Metrics
acc= accuracy_score(y_test,y_pred)
prec = precision_score(y_test,y_pred)
rec = recall_score(y_test,y_pred)
f1 = f1_score(y_test,y_pred)
roc = roc_auc_score(y_test, y_proba)

print(f"TestAccuracy: {acc:.4f}")
print(f"Test Precision: {prec:.4f}")
print(f"Test Recall: {rec:.4f}")
print(f"Test F1-score: {f1:.4f}")
print(f"Test ROC AUC: {roc:.4f}")

print("\n Classification report:")
print(classification_report(y_test,y_pred))

#Confusion matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot= True, fmt = 'd', cmap = 'Blues')
plt.xlabel('predicted')
plt.ylabel('actual')
plt.title(f'Confusion matrix - {best_name}')
plt.show()

"""Step 10 .feature importance

if chosen model supports feature importance (Random Forest/Gradient boosting), show top features
"""

if hasattr(best_pipeline.named_steps['classifier'],'feature_importances_'):
  num_feats = numeric_features
  cat_feats = list(best_pipeline.named_steps['preprocessor'].transformers_[1][1].named_steps['onehot'].get_feature_names_out(categorical_features))
  feature_names = num_feats + cat_feats
  importances = best_pipeline.named_steps['classifier'].feature_importances_
  fi = pd.Series(importances, index = feature_names).sort_values(ascending = False)[:20]
  display(fi)
  plt.figure(figsize = (8,6))
  sns.barplot(x = fi.values, y = fi.index)
  plt.title('Top 20 feature importances')
  plt.show()
else:
  print('Selected model does not provide feature_importances_ attributes')

"""step11. save best pipeline and preprocessing artifacts"""

joblib.dump(best_pipeline,'best_churn_pipeline.pkl')
print("Saved pipeline: best_churn_pipeline.pkl")

"""step 12. Example: predict churn for new customer"""

# --- New customer sample ---
sample = {
    'customer_id': 373292028,
    'credit_score': 650,
    'country': 'France',
    'gender': 'Male',
    'age': 40,
    'tenure': 3,
    'balance': 50000.0,
    'products_number': 2,
    'credit_card': 1,
    'active_member': 1,
    'estimated_salary': 60000.0
}

sample_df = pd.DataFrame([sample])

# --- Apply same feature engineering ---
sample_df['balance_per_product'] = sample_df['balance'] / (sample_df['products_number'].replace(0, np.nan))
sample_df['balance_per_product'].fillna(0, inplace=True)

sample_df['salary_balance_ratio'] = sample_df['estimated_salary'] / (sample_df['balance'].replace(0, np.nan))
sample_df['salary_balance_ratio'].replace([np.inf, -np.inf], np.nan, inplace=True)
sample_df['salary_balance_ratio'].fillna(sample_df['salary_balance_ratio'].median(), inplace=True)

bins = [0,25,35,45,55,65,100]
labels = ['<25','25-34','35-44','45-54','55-64','65+']
sample_df['age_group'] = pd.cut(sample_df['age'], bins=bins, labels=labels)

sample_df['tenure_bucket'] = pd.cut(sample_df['tenure'], bins=[-1,0,2,5,10,100], labels=['0','1-2','3-5','6-10','10+'])
sample_df['high_balance'] = (sample_df['balance'] > 50000.0).astype(int)  # can use 75th percentile of training set


# --- Drop ID ---
sample_df = sample_df.drop(columns=['customer_id'])

# --- Predict ---
pred = best_pipeline.predict(sample_df)[0]
prob = best_pipeline.predict_proba(sample_df)[0,1]

print(f'Predicted churn: {pred}, probability of churn: {prob:.3f}')

